{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fbb17a-9fe9-4d64-936f-d23acfdd9aaa",
   "metadata": {},
   "source": [
    "# Machine learning and deep learning methods applied to predicting customer status\n",
    "\n",
    "Purpose: Predict active and not active clients based on the proposed data structure and naive data structure. Here, we implemented Support Vector Machines (SVM), Random Forest (RF), K-nearest Neighbours (KNN) and Lasso.\n",
    "\n",
    "Author: Gabriel Rodrigues Palma and Rafael de Andrade Moral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfda998-04f0-4f9a-9229-74d26d04aab1",
   "metadata": {},
   "source": [
    "# Packages used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571c5e89-8326-43a7-afcd-10d455740f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# visualisation modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Deep learning modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "# Machine learning packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Additional packages\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import L1\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Testing GPU from MacOs\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3010038-5d9c-4ee5-9fb9-568403ecf885",
   "metadata": {},
   "source": [
    "# Functions used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97976eec-6d63-41ce-b581-fb82316c31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, response_class):\n",
    "    ''' This function reads and prepare the datasets for applying the ML and DL methods'''\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.drop(columns = ['Unnamed: 0', 'Subject'])\n",
    "    explanatory_variables = data.drop(columns = response_class)\n",
    "    response_variables = data[response_class]        \n",
    "    binarizer = LabelBinarizer()\n",
    "    response_variables = binarizer.fit_transform(response_variables)\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    hot_encode_response_variable = onehot_encoder.fit_transform(np.array(response_variables).reshape(-1, 1))          \n",
    "    \n",
    "    return(explanatory_variables, \n",
    "           hot_encode_response_variable, response_variables)\n",
    "\n",
    "def check_zero_division_and_get_rates(cm):\n",
    "    ''' This functions checks for divisions per zeros in the computation of \n",
    "       True and false positive rate based on the confusion matrix array. Also, \n",
    "       this function returns the checked rates'''\n",
    "    \n",
    "    if any(np.sum(cm, axis = 1)==0):            \n",
    "            tpr = cm[:,1][1]/(np.sum(cm, axis = 1)[1]+1e-16)\n",
    "            fpr = cm[:,1][0]/(np.sum(cm, axis = 1)[0]+1e-16)    \n",
    "    else:           \n",
    "        rates = cm[:,1]/np.sum(cm, axis = 1)    \n",
    "        fpr = rates[0]    \n",
    "        tpr = rates[1]\n",
    "    if np.isnan(tpr):        \n",
    "        tpr=0\n",
    "        \n",
    "    return(tpr, fpr)\n",
    "\n",
    "def check_and_compute_rates(predictions, \n",
    "                            classes, \n",
    "                            cm):    \n",
    "    ''' This function returns the values of true and false positive rate for special cases\n",
    "       where the division is not possible to obtain automatically based on the confusion\n",
    "       matrix array provided by sklearn'''\n",
    "        \n",
    "    if (sum(predictions) == 0 and sum(classes) == 0):        \n",
    "        fpr = 0\n",
    "        tpr = 0\n",
    "    elif (sum(predictions) == len(predictions) and sum(classes) == len(classes)):        \n",
    "        fpr = 0\n",
    "        tpr = 1\n",
    "    else:            \n",
    "        tpr, fpr = check_zero_division_and_get_rates(cm)\n",
    "        \n",
    "    return(tpr, fpr)\n",
    "    \n",
    "def get_rates(y_pred,\n",
    "              y_true):\n",
    "    ''' This function get the true and false positive rates based on the \n",
    "       predictied'''\n",
    "            \n",
    "    cm = confusion_matrix(y_true = y_true, y_pred = y_pred)\n",
    "    tpr, fpr = check_and_compute_rates(y_pred, y_true, cm)\n",
    "        \n",
    "    return(tpr, fpr)\n",
    "\n",
    "def get_rates_by_cross_validation(raw_data):\n",
    "    ''' This function obtains the accuracy, true and false positive rates \n",
    "       based on the cross k-fold cross validation'''\n",
    "            \n",
    "    for train_index, test_index in KFold(n_splits=5, shuffle=True).split(patterns):\n",
    "        \n",
    "        x_train, x_test = patterns[train_index], patterns[test_index]\n",
    "        y_train, y_test = classes[train_index], classes[test_index]\n",
    "                \n",
    "        pbp_predictions = pbp_prediction(patterns_array=x_test, \n",
    "                                         clustered_patterns = clustered_patterns, \n",
    "                                         d_base = d_base, alpha = alpha, \n",
    "                   outbreak_p_means = prediction.obtain_p_means_with_distance,\n",
    "                   outbreak_prediction = prediction.predict_with_distance)\n",
    "        rates = get_rates(predictions = pbp_predictions, classes = y_test)\n",
    "        tpr.append(rates[0])\n",
    "        fpr.append(rates[1])\n",
    "    \n",
    "    return(np.mean(tpr[tpr!=np.nan]), np.mean(fpr[fpr!=np.nan]))\n",
    "\n",
    "def get_statistics(y_pred, y_true):\n",
    "    ''' This function obtains the following statistics: Accuracy, True positive rate and \n",
    "       False positive rate'''\n",
    "\n",
    "    auc = roc_auc_score(y_score = y_pred,\n",
    "              y_true = y_true)\n",
    "    acc = accuracy_score(y_pred = y_pred,\n",
    "                         y_true = y_true)\n",
    "    rates = get_rates(y_pred = y_pred,\n",
    "              y_true = y_true)\n",
    "    tpr = rates[0]\n",
    "    fpr = rates[1]\n",
    "    return([acc, tpr, fpr, auc])\n",
    "\n",
    "# def get_methods_performance(X_train, X_test, \n",
    "#                             y_train, y_test, \n",
    "#                             raw_data):\n",
    "#     ''' This function obtain the performance of each selected model'''\n",
    "#     kf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "#     # Support Vector Machine -----\n",
    "#     ## Non-linear\n",
    "#     nonlinear_svm = svm.NuSVC(gamma=\"auto\")\n",
    "#     #Â Create the parameter space\n",
    "#     params = {\"kernel\": ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "#               \"gamma\": ['auto', 'scale'],\n",
    "#              \"degree\": np.arange(1, 10, 1)}\n",
    "#     nonlinear_svm_cv = RandomizedSearchCV(nonlinear_svm, params, cv=kf)\n",
    "#     nonlinear_svm_cv.fit(X_train, y_train)\n",
    "#     nonlinear_svm_predictions = nonlinear_svm_cv.predict(X_test)\n",
    "    \n",
    "#     ## Polinomial\n",
    "#     polinomial_svm = svm.SVC(kernel='poly', degree = 3)\n",
    "#     polinomial_svm.fit(X_train, y_train)\n",
    "#     polinomial_svm_predictions = polinomial_svm.predict(X_test)\n",
    "    \n",
    "#     # Random forest -----\n",
    "#     rf = RandomForestClassifier(max_depth=5, random_state=0, n_estimators = 1000)\n",
    "#     rf.fit(X_train, y_train)\n",
    "#     random_forest_predictions = rf.predict(X_test)\n",
    "    \n",
    "#     # KNN -----\n",
    "#     knn = KNeighborsClassifier(n_neighbors=1)\n",
    "#     knn.fit(X_train, y_train)\n",
    "#     knn_predictions = knn.predict(X_test)\n",
    "    \n",
    "#     # Lasso -----\n",
    "#     lasso = Lasso(alpha=0.02)\n",
    "#     lasso.fit(X_train, y_train)\n",
    "#     lasso_predictions = np.round(lasso.predict(X_test))\n",
    "    \n",
    "#     # Deep Neural Network -----\n",
    "# #     if raw_data==True:\n",
    "# #         dnn = keras.models.load_model('DNN/DNN_model_params.h5')\n",
    "# #         dnn_predictions = np.round(dnn.predict(X_test))\n",
    "# #     elif raw_data == 'combined':\n",
    "# #         dnn = keras.models.load_model('DNN/DNN_model_fulldata.h5')\n",
    "# #         dnn_predictions = np.round(dnn.predict(X_test))\n",
    "# #     else:    \n",
    "# #         dnn = keras.models.load_model('DNN/DNN_model_hmm.h5')\n",
    "# #         dnn_predictions = np.round(dnn.predict(X_test))\n",
    "\n",
    "    \n",
    "#     # Obtaining statistics -----\n",
    "#     polynomial_svm_performance = get_statistics(y_pred = polinomial_svm_predictions,\n",
    "#                                         y_true = y_test)\n",
    "#     nonlinear_svm_performance = get_statistics(y_pred = nonlinear_svm_predictions,\n",
    "#                                        y_true = y_test)\n",
    "#     random_forest_performance = get_statistics(y_pred = random_forest_predictions,\n",
    "#                                        y_true = y_test)\n",
    "#     knn_performance = get_statistics(y_pred = knn_predictions,\n",
    "#                                        y_true = y_test)    \n",
    "#     lasso_performance = get_statistics(y_pred = lasso_predictions,\n",
    "#                                        y_true = y_test)\n",
    "# #     dnn_performance = get_statistics(y_pred = dnn_predictions,\n",
    "# #                                        y_true = y_test)\n",
    "\n",
    "#     return(polynomial_svm_performance, nonlinear_svm_performance, \n",
    "#            random_forest_performance, knn_performance, \n",
    "#            lasso_performance)\n",
    "    \n",
    "# def get_results_data(explanatory_variables,                             \n",
    "#                      response_variables,                                                         \n",
    "#                      test_size, raw_data):\n",
    "#     ''' This function obtain the performance of the ML and DL methods based on\n",
    "#        the prediction of client status'''\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(explanatory_variables, \n",
    "#                                                         response_variables, \n",
    "#                                                         test_size = test_size, \n",
    "#                                                         random_state = 42)\n",
    "   \n",
    "#     # Obtaining methods performance\n",
    "#     polynomial_svm_performance, nonlinear_svm_performance, \\\n",
    "#     random_forest_performance, knn_performance, \\\n",
    "#     lasso_performance = get_methods_performance(X_train, X_test, \n",
    "#                                               y_train, y_test, \n",
    "#                                               raw_data)\n",
    "        \n",
    "    \n",
    "#     model_outputs_performance = pd.DataFrame({'Statistics':['Accuracy', \n",
    "#                                                             'True Positive Rate', \n",
    "#                                                             'False Positive Rate', \n",
    "#                                                             'AUROC'],\n",
    "#                                               'Polinomial SVM': [polynomial_svm_performance[0], \n",
    "#                                                                  polynomial_svm_performance[1], \n",
    "#                                                                  polynomial_svm_performance[2],\n",
    "#                                                                 polynomial_svm_performance[3]], \n",
    "#                                               'Non linear SVM': [nonlinear_svm_performance[0], \n",
    "#                                                                  nonlinear_svm_performance[1], \n",
    "#                                                                  nonlinear_svm_performance[2], \n",
    "#                                                                 nonlinear_svm_performance[3]], \n",
    "#                                               'Random Forest': [random_forest_performance[0], \n",
    "#                                                                 random_forest_performance[1], \n",
    "#                                                                 random_forest_performance[2], \n",
    "#                                                                random_forest_performance[3]], \n",
    "#                                               'KNN': [knn_performance[0], \n",
    "#                                                       knn_performance[1], \n",
    "#                                                       knn_performance[2], \n",
    "#                                                       knn_performance[3]],                                                \n",
    "#                                               'Lasso': [lasso_performance[0], \n",
    "#                                                         lasso_performance[1], \n",
    "#                                                         lasso_performance[2], \n",
    "#                                                         lasso_performance[3]]})\n",
    "\n",
    "#     return(model_outputs_performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006f4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_methods_predictions(X_train, X_test, \n",
    "                            y_train, y_test, \n",
    "                            data_type):\n",
    "    ''' This function obtain the performance of each selected model'''\n",
    "    kf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    # Support Vector Machine -----\n",
    "    ## Non-linear\n",
    "    nonlinear_svm = svm.NuSVC()\n",
    "    \n",
    "    nonlinear_svm.fit(X_train, y_train)\n",
    "    \n",
    "    nonlinear_svm_predictions = nonlinear_svm.predict(X_test)\n",
    "    \n",
    "    ## Polinomial\n",
    "    polinomial_svm = svm.SVC(kernel='poly', degree = 3)\n",
    "    polinomial_svm.fit(X_train, y_train)\n",
    "    polinomial_svm_predictions = polinomial_svm.predict(X_test)\n",
    "    \n",
    "    # Random forest -----\n",
    "    rf = RandomForestClassifier(max_depth=5, random_state=0, n_estimators = 1000)\n",
    "    rf.fit(X_train, y_train)\n",
    "    random_forest_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # KNN -----\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_predictions = knn.predict(X_test)\n",
    "    \n",
    "    # Lasso -----\n",
    "    lasso = Lasso(alpha=0.02)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    lasso_predictions = np.round(lasso.predict(X_test))\n",
    "    \n",
    "    # Deep Neural Network -----\n",
    "\n",
    "    dnn = Sequential()\n",
    "    dnn.add(Dense(100, input_shape=(4,)))\n",
    "    dnn.add(Dense(150, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(200, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(150, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(46, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(20, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(10, activation = LeakyReLU(alpha=0.4)))\n",
    "    dnn.add(Dense(1, activation = 'sigmoid'))\n",
    "    dnn.compile(optimizer = 'adam', \n",
    "                  loss = 'mse', \n",
    "                 metrics = ['accuracy', 'TruePositives', 'FalsePositives', 'FalseNegatives', 'TrueNegatives'])\n",
    "    dnn.fit(X_train, y_train, batch_size = 10, epochs=100, verbose = False)\n",
    "    dnn_predictions = np.round(dnn.predict(X_test))            \n",
    "\n",
    "    return(polinomial_svm_predictions, nonlinear_svm_predictions, \n",
    "           random_forest_predictions, knn_predictions, \n",
    "           lasso_predictions, dnn_predictions)\n",
    "\n",
    "def get_methods_predictions_loo(polinomial_svm_predictions, nonlinear_svm_predictions, \n",
    "                                random_forest_predictions, knn_predictions, \n",
    "                                lasso_predictions, dnn_predictions, \n",
    "                                reponse_variable):\n",
    "    \n",
    "    '''This function obtain the methods performance based on their predictions for the leave one out cross valition'''\n",
    "    polynomial_svm_performance = get_statistics(y_pred = polinomial_svm_predictions,\n",
    "                                    y_true = reponse_variable)\n",
    "    nonlinear_svm_performance = get_statistics(y_pred = nonlinear_svm_predictions,\n",
    "                                       y_true = reponse_variable)\n",
    "    random_forest_performance = get_statistics(y_pred = random_forest_predictions,\n",
    "                                       y_true = reponse_variable)\n",
    "    knn_performance = get_statistics(y_pred = knn_predictions,\n",
    "                                       y_true = reponse_variable)    \n",
    "    lasso_performance = get_statistics(y_pred = lasso_predictions,\n",
    "                                       y_true = reponse_variable)\n",
    "    dnn_performance = get_statistics(y_pred = dnn_predictions,\n",
    "                                        y_true = reponse_variable)\n",
    "    return(polynomial_svm_performance, nonlinear_svm_performance, \n",
    "           random_forest_performance, knn_performance, \n",
    "           lasso_performance, dnn_performance)\n",
    "\n",
    "    \n",
    "def get_results_data_loo(explanatory_variables,                         \n",
    "                         response_variables,                                                         \n",
    "                         data_type):\n",
    "    ''' This function obtain the performance of the ML and DL methods based on\n",
    "       the prediction of client status based on leave one out cross validation'''\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(explanatory_variables)\n",
    "    polinomial_svm_predictions, nonlinear_svm_predictions, \\\n",
    "           random_forest_predictions, knn_predictions, \\\n",
    "           lasso_predictions, dnn_predictions = ([], [], [], [], [], [])\n",
    "    \n",
    "    for train_index, test_index in loo.split(explanatory_variables):\n",
    "        X_train, X_test = explanatory_variables[train_index], explanatory_variables[test_index]\n",
    "        y_train, y_test = response_variables[train_index], response_variables[test_index]                \n",
    "             \n",
    "        ## Correcting errors related to column vector (n_observations, ) to (, n_observations)\n",
    "        y_train = y_train.ravel() \n",
    "        y_test = y_test.ravel()        \n",
    "    \n",
    "        polynomial_svm_prediction, nonlinear_svm_prediction, \\\n",
    "        random_forest_prediction, knn_prediction, \\\n",
    "        lasso_prediction, dnn_prediction = get_methods_predictions(X_train, X_test, \n",
    "                                                   y_train, y_test, \n",
    "                                                   data_type)\n",
    "        \n",
    "        polinomial_svm_predictions.append(polynomial_svm_prediction[0])\n",
    "        nonlinear_svm_predictions.append(nonlinear_svm_prediction[0])\n",
    "        random_forest_predictions.append(random_forest_prediction[0])\n",
    "        knn_predictions.append(knn_prediction[0])        \n",
    "        lasso_predictions.append(lasso_prediction[0]) \n",
    "        dnn_predictions.append(dnn_prediction[0])\n",
    " \n",
    "    polynomial_svm_performance, nonlinear_svm_performance, \\\n",
    "       random_forest_performance, knn_performance, \\\n",
    "       lasso_performance, dnn_performance = get_methods_predictions_loo(polinomial_svm_predictions, nonlinear_svm_predictions, \n",
    "                            random_forest_predictions, knn_predictions, \n",
    "                            lasso_predictions, dnn_predictions, \n",
    "                            list(response_variables.flatten()))\n",
    "    \n",
    "    model_outputs_performance = pd.DataFrame({'Statistics':['Accuracy', \n",
    "                                                            'True Positive Rate', \n",
    "                                                            'False Positive Rate', \n",
    "                                                            'AUROC'],\n",
    "                                              'Polinomial SVM': [polynomial_svm_performance[0], \n",
    "                                                                 polynomial_svm_performance[1], \n",
    "                                                                 polynomial_svm_performance[2],\n",
    "                                                                polynomial_svm_performance[3]], \n",
    "                                              'Non linear SVM': [nonlinear_svm_performance[0], \n",
    "                                                                 nonlinear_svm_performance[1], \n",
    "                                                                 nonlinear_svm_performance[2], \n",
    "                                                                nonlinear_svm_performance[3]], \n",
    "                                              'Random Forest': [random_forest_performance[0], \n",
    "                                                                random_forest_performance[1], \n",
    "                                                                random_forest_performance[2], \n",
    "                                                               random_forest_performance[3]], \n",
    "                                              'KNN': [knn_performance[0], \n",
    "                                                      knn_performance[1], \n",
    "                                                      knn_performance[2], \n",
    "                                                      knn_performance[3]],                                                \n",
    "                                              'Lasso': [lasso_performance[0], \n",
    "                                                        lasso_performance[1], \n",
    "                                                        lasso_performance[2], \n",
    "                                                        lasso_performance[3]], \n",
    "                                             'DNN': [dnn_performance[0], \n",
    "                                                        dnn_performance[1], \n",
    "                                                        dnn_performance[2], \n",
    "                                                        dnn_performance[3]]})\n",
    "\n",
    "    return(model_outputs_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ddb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class(lime_importance):\n",
    "    '''This function classify the class among control or experiment based on the LIME score\n",
    "    negative scores relates to control and positive experiment'''\n",
    "    if (lime_importance < 0):\n",
    "        return('Control')\n",
    "    else:\n",
    "        return('Experiment')\n",
    "def get_lime_importance(explanatory_variables, \n",
    "                        response_variables, \n",
    "                        trial_name, num_features, \n",
    "                        feature_names):\n",
    "    '''This function obtains the parameters importance based on the LIME method. For that the leave one out cross \n",
    "    validation approach was used in order to obtain parameter importance for each subject of the experiment'''\n",
    "    parameters = []\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(explanatory_variables)\n",
    "    \n",
    "    \n",
    "    for train_index, test_index in loo.split(explanatory_variables):\n",
    "        X_train, X_test = explanatory_variables[train_index], explanatory_variables[test_index]\n",
    "        y_train, y_test = response_variables[train_index], response_variables[test_index]   \n",
    "        \n",
    "        predict_fn_rf = lambda x: rf.predict_proba(x).astype(float)\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(X_train, \n",
    "                                                           feature_names = feature_names,\n",
    "                                                           class_names=['Control', 'Experiment'], \n",
    "                                                           verbose = False, mode='classification')\n",
    "\n",
    "        exp = explainer.explain_instance(X_test[0], predict_fn_rf, num_features=4)\n",
    "        lime_results = exp.as_list()\n",
    "        for feature_index in range(num_features):\n",
    "            lime_feature_importance = list(lime_results[feature_index])\n",
    "            lime_feature_importance.append(check_class(list(lime_results[feature_index])[1]))\n",
    "            lime_feature_importance.append(trial_name)\n",
    "            parameters.append(lime_feature_importance)\n",
    "    parameters = pd.DataFrame(lime_results_Trial_1, columns = ['Features', 'Local \\n Importance', 'Class', 'Trial'])\n",
    "        \n",
    "    return(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601b56e-f379-4b23-9d9f-00e7fab57c4b",
   "metadata": {},
   "source": [
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5a00c",
   "metadata": {},
   "source": [
    "Before obtaining the performance of the learning algorithms selected for this paper, we need to import the datasets related to the Hidden Markov Models features and the peak features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c84279",
   "metadata": {},
   "source": [
    "## Coordinate data Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "118ff218",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_explanatory_variables_t1, \\\n",
    "  coordinates_hot_encode_response_variable_t1, coordinates_response_variable_t1 = create_dataset(path = '../../output_data/New_data/CoordinateDatasets/RawMidlineCoordinates_data_Trial1.csv', \n",
    "                                                                                        response_class = 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076fd953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dis_sum</th>\n",
       "      <th>angle_sum</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>idle_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.328422</td>\n",
       "      <td>1444.039766</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.301916</td>\n",
       "      <td>909.662506</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.007659</td>\n",
       "      <td>651.069516</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.350488</td>\n",
       "      <td>1406.028426</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.195562</td>\n",
       "      <td>882.929882</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dis_sum    angle_sum  average_speed  idle_time\n",
       "0  4.328422  1444.039766       0.002537       1.50\n",
       "1  4.301916   909.662506       0.002402       1.75\n",
       "2  4.007659   651.069516       0.001659       4.50\n",
       "3  4.350488  1406.028426       0.002660       1.25\n",
       "4  4.195562   882.929882       0.002113       2.50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_explanatory_variables_t1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ed1e7",
   "metadata": {},
   "source": [
    "## Raw Midline EEG data Trial 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56a249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_explanatory_variables_t12, \\\n",
    "  coordinates_hot_encode_response_variable_t12, coordinates_response_variable_t12 = create_dataset(path = '../../output_data/New_data/CoordinateDatasets/RawMidlineCoordinates_data_Trial12.csv', \n",
    "                                                                                        response_class = 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01a6ca7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dis_sum</th>\n",
       "      <th>angle_sum</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>idle_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.750087</td>\n",
       "      <td>276.780542</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.701265</td>\n",
       "      <td>241.570413</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.610205</td>\n",
       "      <td>225.982551</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753753</td>\n",
       "      <td>240.335305</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496671</td>\n",
       "      <td>93.845292</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dis_sum   angle_sum  average_speed  idle_time\n",
       "0  0.750087  276.780542       0.009918       0.50\n",
       "1  0.701265  241.570413       0.007796       1.00\n",
       "2  0.610205  225.982551       0.007746       0.75\n",
       "3  0.753753  240.335305       0.009937       0.50\n",
       "4  0.496671   93.845292       0.003795       2.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_explanatory_variables_t12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9981f",
   "metadata": {},
   "source": [
    "# Obtaining learning algorithms performance based on Leave one out cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69719034",
   "metadata": {},
   "source": [
    "## Coordinate data Trial 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b520e10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_3/hy1xvl1s545g3884zjn6jhhm0000gn/T/ipykernel_36291/1798970732.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lime_results_Trial_1 = get_lime_importance(explanatory_variables = np.array(coordinates_explanatory_variables_t1),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                            \u001b[0mresponse_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinates_response_variable_t1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                            \u001b[0mtrial_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Trial 1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                            \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                            feature_names = coordinates_explanatory_variables_t1.columns.values.tolist())\n",
      "\u001b[0;32m/var/folders/_3/hy1xvl1s545g3884zjn6jhhm0000gn/T/ipykernel_36291/3691232299.py\u001b[0m in \u001b[0;36mget_lime_importance\u001b[0;34m(explanatory_variables, response_variables, trial_name, num_features, feature_names)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpredict_fn_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         explainer = lime.lime_tabular.LimeTabularExplainer(X_train, \n\u001b[0m\u001b[1;32m     25\u001b[0m                                                            \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                            \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Control'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Experiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lime' is not defined"
     ]
    }
   ],
   "source": [
    "lime_results_Trial_1 = get_lime_importance(explanatory_variables = np.array(coordinates_explanatory_variables_t1),\n",
    "                                           response_variables = coordinates_response_variable_t1, \n",
    "                                           trial_name = 'Trial 1',\n",
    "                                           num_features = 4, \n",
    "                                           feature_names = coordinates_explanatory_variables_t1.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75c661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5d2bab",
   "metadata": {},
   "source": [
    "## Coordinate data Trial 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_results_Trial_12 = get_lime_importance(explanatory_variables = np.array(coordinates_explanatory_variables_t12),\n",
    "                                           response_variables = coordinates_response_variable_t12, \n",
    "                                           trial_name = 'Trial 12',\n",
    "                                           num_features = 4, \n",
    "                                           feature_names = coordinates_explanatory_variables_t1.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46332d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
